version: '3'

networks:
  traefik:
    external: true

# description: Eeasy way to run large language models locally - Nvidia GPU
# https://hub.docker.com/r/ollama/ollama
# https://github.com/ollama/ollama

# add models to etc/ollama/Ollamamodels and run 
# make.d/scripts/ollama-update-models.sh
# to pull or update models
# see .templates/Ollamamodels.sample for examples
# Available models: https://ollama.com/library/


services:
  ollama:
    image: ollama/ollama:${OLLAMA_DOCKER_TAG:-latest}
    container_name: ${OLLAMA_CONTAINER_NAME:-ollama}
    restart: ${OLLAMA_RESTART:-unless-stopped}
    networks:
      - traefik
    volumes:
      - ${OLLAMA_DATA_PATH:-./media/ollama}:/root/.ollama
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ}
      - gpus=${OLLAMA_GPUS:-all}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0'] # this is only needed when using multiple GPUs
              #count: 1 # number of GPUs
              capabilities: [gpu]
    labels:
      - joyride.host.name=${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}
      - traefik.enable=true
      - traefik.http.routers.ollama.entrypoints=websecure
      - traefik.http.routers.ollama.rule=Host(`${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}`)
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - com.centurylinklabs.watchtower.enable=true
      - autoheal=true
