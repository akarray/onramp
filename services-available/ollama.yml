version: '3'

networks:
  traefik:
    external: true

volumes:
  ollama:

# https://hub.docker.com/r/ollama/ollama
# https://github.com/ollama/ollama
# https://github.com/ollama/ollama-python

# docker exec -it ollama ollama run llama2    # https://ollama.com/library/llama2     [4 GB]
# docker exec -it ollama ollama run llama2:chat     [4 GB]
# docker exec -it ollama ollama run llama2:13b-chat [7.5 GB]
# docker exec -it ollama ollama run mistral   # https://ollama.com/library/mistral    [4 GB]
# docker exec -it ollama ollama run mixtral   # https://ollama.com/library/mixtral    [26 GB]
# docker exec -it ollama ollama run codellama # https://ollama.com/library/codellama  [4 GB]
# docker exec -it ollama ollama run openhermes [4 GB]

services:
  ollama:
    image: ollama/ollama:${OLLAMA_DOCKER_TAG:-latest}
    container_name: ${OLLAMA_CONTAINER_NAME:-ollama}
    restart: ${OLLAMA_RESTART:-unless-stopped}
    networks:
      - traefik
    volumes:
      - ollama:/root/.ollama
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ}
      - gpus=${OLLAMA_GPUS:-all}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0'] # this is only needed when using multiple GPUs
              #count: 1 # number of GPUs
              capabilities: [gpu]
    labels:
      - joyride.host.name=${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}
      - traefik.enable=true
      - traefik.http.routers.ollama.entrypoints=websecure
      - traefik.http.routers.ollama.rule=Host(`${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}`)
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - com.centurylinklabs.watchtower.enable=true
      - autoheal=true
